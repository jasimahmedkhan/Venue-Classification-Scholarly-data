{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author : Jasim Ahmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import nltk\n",
    "import nltk as nk\n",
    "from nltk import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "import mlxtend\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics \n",
    "\n",
    "# Accessing Files and Folders\n",
    "# import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NLTK and mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U nltk\n",
    "# !pip install mlxtend\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading JSON and removing null instances in rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://lfs.aminer.cn/misc/dblp.v11.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Dataset and dropping rows on basis of NAN value occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\"dblp.v11.json\",lines=True)\n",
    "data.dropna(axis=0,how=\"any\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify whether all the venues are consistent with two values (id and raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venRaw = 0\n",
    "for count,entry in enumerate(data[\"venue\"]):\n",
    "    if (\"raw\" not in entry.keys()):\n",
    "        print(count)\n",
    "        venRaw = count\n",
    "\n",
    "data.drop(index=data.index[venRaw],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokenized form of the abstract(tokenization at word level in a sentence) and the original abstract.\n",
    "def Tokenized_and_OriginalAbstract(data):\n",
    "    \"\"\"\n",
    "        Input data: String\n",
    "        return: tokenized words at sentence level and other is a String \n",
    "    \"\"\"\n",
    "    x = lambda x:x[\"InvertedIndex\"].keys()\n",
    "    tokenized_abstract = [list(x(entry)) for entry in data] # tokeized form of the abstract content\n",
    "    original_abstract = [\" \".join(entry) for entry in tokenized_abstract]\n",
    "    return (tokenized_abstract,original_abstract)\n",
    "\n",
    "# get venues from the dataset\n",
    "def Venues(data):\n",
    "    \"\"\"\n",
    "        Input data: list of dictionary of raw venue and their ids\n",
    "        return: venues as a string \n",
    "    \"\"\"\n",
    "    x = lambda x:x[\"raw\"]\n",
    "    return [x(entry) for entry in data]\n",
    "\n",
    "# Authors and Field of Study(Keywords)\n",
    "def Author_and_FOS_Values(data):\n",
    "    \"\"\"\n",
    "        Input data: String\n",
    "        return: authors and FOS as string \n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    for entry in data:\n",
    "        valueInEntry = []\n",
    "        for value in entry:\n",
    "            valueInEntry.append(value[\"name\"])\n",
    "#       filtered_data.append(valueInEntry)\n",
    "        filtered_data.append(\", \".join(valueInEntry))\n",
    "    return filtered_data \n",
    "\n",
    "### Initilazations and value setting\n",
    "authors = Author_and_FOS_Values(data[\"authors\"]) # Author Names of a particulat research paper\n",
    "keywords = Author_and_FOS_Values(data[\"fos\"]) # Field of Study of research paper\n",
    "abstract = Tokenized_and_OriginalAbstract(data[\"indexed_abstract\"]) # Abstract of the research paper\n",
    "title = data[\"title\"] # title of the research paper\n",
    "year = data[\"year\"] # year in which the paper was published\n",
    "venue = Venues(data[\"venue\"])\n",
    "\n",
    "data = pd.DataFrame(data=list(zip(authors,keywords,abstract[1],title,year,venue)),columns=[\"authors\",\"fos\",\"abstract\",\"title\",\"year\",\"venue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to save the data for later use because of its volume and variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"dblp_paper_total.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved data as CSV, merely a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### pd.read_csv(\"dblp_paper_total.csv\",error_bad_lines=False) # if bad lines occurs\n",
    "# data = pd.read_csv(\"dblp_paper_total.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the venues as a center attribute to group and extract papers that are in a particular venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topVenues = data.groupby(by=\"venue\").size().sort_values(ascending=False)[:200]\n",
    "venues_To_Extract = topVenues.index.to_list()[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to the plot the elbow of the venue is some where around 35 and 55. This value will help later on in classifying the papers in a particular venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(topVenues.values)\n",
    "plt.ylabel('Number of Published Papers')\n",
    "plt.xlabel('Top Venues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the optimal venue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venueList = pd.read_csv(\"venueList_To_Extract.csv\")\n",
    "# venues_To_Extract = [\"\".join(entry) for entry in venueList.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_data = pd.DataFrame()\n",
    "all_data = pd.DataFrame()\n",
    "for venue in venues_To_Extract:\n",
    "    temp_df = data.iloc[data.index[data[\"venue\"] == venue].tolist()].reset_index(drop=True)\n",
    "    all_data = all_data.append(temp_df,ignore_index=True,sort=False)\n",
    "\n",
    "cummulative_data = cummulative_data.append(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomly shuffle the data and reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_data = cummulative_data.sample(frac=1).reset_index(drop=True)\n",
    "cummulative_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting and saving all the research papers that exist in the list of venue_To_Extract. So, that are data is skewed and there are no outliers(records of research papers) of any other non-existing venue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_cummulative_data = pd.DataFrame(data=list(zip(cummulative_data[\"authors\"],cummulative_data[\"fos\"],cummulative_data[\"abstract\"],cummulative_data[\"title\"],cummulative_data[\"year\"],cummulative_data[\"venue\"])),columns=[\"authors\",\"fos\",\"abstract\",\"title\",\"year\",\"venue\"])\n",
    "# save_cummulative_data.to_csv(\"cummulative_data_150k.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether venues exit in that number or not\n",
    "# all_data.iloc[all_data[all_data[\"venue\"] == \"international conference on parallel processing\"].index.tolist()]\n",
    "\n",
    "# # check whether the collection of the data from the dataset is right\n",
    "# cummulative_data.groupby(by=\"venue\").size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics on the subset of dataset that is normailzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the original data as well as the cummulative data for inconsistencies and NAN values\n",
    "data.iloc[data.index[data[\"venue\"] == \"international conference on parallel processing\"].tolist()].reset_index(drop=True)\n",
    "# data.iloc[data.loc[pd.isnull(data).any(1), :].index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cummulative_data.iloc[cummulative_data.loc[pd.isnull(cummulative_data).any(1), :].index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic statistics on the categorical data to find the unique and most occurred values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_data.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of data over the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "cummulative_data['year'].plot(linewidth=1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cummulative_data[\"year\"]) # distribution of data over the year using histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_data.boxplot() # check whether the outliers occur in which particular window(years) using Boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of research papers in a particular venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "venuesList = data.groupby(by=\"venue\").size().sort_values(ascending=False)[:100]\n",
    "venuesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which year had which papers and at which venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.groupby(by=[\"year\",\"venue\"]).size()[::-1][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a particular venue how many papers were published given an year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(by=[\"year\",\"venue\"]).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumberOfPapersInVenue_OR_FOS(fromTime, toTime):\n",
    "    \"\"\"\n",
    "        Description: To find the number of research papers published in a particular time frame either in a venue \n",
    "        or field of study\n",
    "        Input fromTime and toTime: Number Int\n",
    "        return: text string with its numberic count\n",
    "    \"\"\"\n",
    "    time = cummulative_data[\"year\"].apply(lambda x: x > fromTime and x < toTime )\n",
    "    return cummulative_data[\"venue\"][time].value_counts()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NumberOfPapersInVenue_OR_FOS(1903,2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "NumberOfPapersInVenue_OR_FOS(1900,2000).plot(kind=\"barh\",title=\"Number Of Papers Published In a particular Venue\") #plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Authors with the most number of published papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "data[\"authors\"].value_counts()[:10].plot(kind=\"barh\",title=\"Authors having the most number of published papers\") #plot\n",
    "## another way to do this\n",
    "# data.groupby(by=\"authors\").size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time and the venue of particular authors who were the most active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.in1d(cummulative_data[\"authors\"],[\"John K. Debenham\"]) # when the papers were published and in which venue by people who were most active\n",
    "entries = cummulative_data.index[mask]\n",
    "cummulative_data.iloc[entries]\n",
    "## or in one line\n",
    "# df = data.iloc[data.index[data[\"authors\"] == \"John K. Debenham\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most research papers published in a particular field of study (fos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"fos\"].value_counts()[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "cummulative_data[\"fos\"].value_counts()[:11].plot(kind=\"barh\",title=\"Most papers published in a particular field of study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Paper published in the field of Philosophy, Performance art having venue types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philosophy = (data[\"fos\"] == \"Multimedia, Human–computer interaction, Computer science\")\n",
    "philo = data[philosophy]\n",
    "philo[\"venue\"].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() # For word lemmatization\n",
    "stemmer = PorterStemmer() # For word Stemming\n",
    "REPLACE_BY_SPACE = re.compile('[/(){}\\[\\]\\|@,;]') \n",
    "BAD_SYMBOLS = re.compile('[^0-9a-z #+_]')\n",
    "REMOVING_NUMBERS = re.compile(\"(^|\\W)\\d+\")\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def Nltk2Word_And_Tag(nltk_tag):\n",
    "    \"\"\"\n",
    "        Input text: a string\n",
    "        return: string tag such as 'a','v','n','r'\n",
    "    \"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.wordnet.ADV\n",
    "    else:        \n",
    "        return None\n",
    "    \n",
    "def Lemmatize_Sentence(sentence):\n",
    "    \"\"\"\n",
    "        Input text: a string \n",
    "        return: lemmatized string\n",
    "    \"\"\"\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    w_n_tagged = map(lambda x: (x[0], Nltk2Word_And_Tag(x[1])), nltk_tagged)\n",
    "    res_words = []\n",
    "    for word, tag in w_n_tagged:\n",
    "        if tag is None: \n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(res_words)\n",
    "\n",
    "def Stem_Sentence(sentence):\n",
    "    \"\"\"\n",
    "        Input text: a string\n",
    "        Description: Can be applied for varying the analysis of Classifier\n",
    "        return: stemmed string\n",
    "    \"\"\"\n",
    "    tokenize_words = nltk.word_tokenize(sentence)\n",
    "    stem_sents = [stemmer.stem(word) for word in tokenize_word]\n",
    "    return \" \".join(stem_sents)\n",
    "\n",
    "def Length_Words_Disapproved(sentence,length):\n",
    "    \"\"\"\n",
    "        Input text: a string and int for defining the limit on the length of the words that will be allowed\n",
    "        return: modified text string\n",
    "    \"\"\"\n",
    "    tokenize_words = nltk.word_tokenize(sentence)\n",
    "    sent = list(filter(lambda x: len(x) > length,tokenize_words))\n",
    "    return \" \".join(sent) \n",
    "\n",
    "def Clean_Text(text,flag):\n",
    "    \"\"\"\n",
    "        Input text: a string and flag for stemming on the current text\n",
    "        return: modified text string which is lower-cased\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REMOVING_NUMBERS.sub(\" \", text) # removes the occurences of number such as 2019 or 3valued or 21\n",
    "    text = BAD_SYMBOLS.sub(\" \", text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = REPLACE_BY_SPACE.sub(\" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text \n",
    "    if(flag):\n",
    "        text = Stem_Sentence(text)\n",
    "    \n",
    "    return Lemmatize_Sentence(Length_Words_Disapproved(text,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmatized_abstract = cummulative_data[\"abstract\"].apply(lambda x:Clean_Text(x,0))\n",
    "lemmatized_title = cummulative_data[\"title\"].apply(lambda x:Clean_Text(x,0))\n",
    "lemmatized_fos = cummulative_data[\"fos\"].apply(lambda x:Clean_Text(x,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = nltk.word_tokenize(\" \".join([\" \".join(entry) for entry in np.column_stack((lemmatized_title,lemmatized_abstract)).tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Finding most common terms in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(vocabulary)\n",
    "fdist.plot(5000, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(vocabulary)\n",
    "words = fdist.most_common(5000)\n",
    "vocabulary_terms = []\n",
    "\n",
    "for word_tuple in words:\n",
    "    if(len(word_tuple[0]) > 3):\n",
    "        vocabulary_terms.append(word_tuple[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [\" \".join(entry) for entry in np.column_stack((lemmatized_title,lemmatized_abstract,lemmatized_fos)).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer using Vocabulary and Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer = vectorizer.fit(vocabulary_terms)\n",
    "mat = vectorizer.transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = pd.DataFrame(mat.todense(),columns = vectorizer.get_feature_names())\n",
    "TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction using Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = cummulative_data[\"venue\"].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(TFIDF, y_labels, test_size=0.10, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearSVC_clf = LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)\n",
    "LinearSVC_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clf.coef_)\n",
    "# print(clf.intercept_)\n",
    "y_predict_LinearSVC = LinearSVC_clf.predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test,y_predict_LinearSVC)\n",
    "print(\"classification score:\", LinearSVC_clf.score(X_test,y_test))\n",
    "\n",
    "# print(\"accuracy:\", metrics.accuracy_score(y_test,y_predict_LinearSVC))\n",
    "# print(\"precision:\", metrics.precision_score(y_test,y_predict_LinearSVC, average=\"micro\"))\n",
    "# print(\"recall:\", metrics.recall_score(y_test,y_predict_LinearSVC, average=\"micro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.afmhot)\n",
    "classNames = list(set(y_labels))\n",
    "ax = %matplotlib.plt.ad(111)\n",
    "cax = ax.matshow(cm)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "plt.title('SVM RBF Kernel Confusion Matrix - Test Data')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, rotation=45)\n",
    "plt.yticks(tick_marks)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cross Validation KFold and RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilization\n",
    "classifier_Multinomial = MultinomialNB()\n",
    "classifier_LogisticRegression = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n",
    "classifier_RandomForestClassifier = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "classifier_LinearSVC_Classifier = LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)\n",
    "\n",
    "# Setting values for Kfold and repeated Kflod\n",
    "kf = KFold(10,True,1)\n",
    "random_state = 300\n",
    "# rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None) \n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=2, random_state=random_state)\n",
    "\n",
    "classifier_List = [classifier_Multinomial,\n",
    "                   classifier_LogisticRegression,\n",
    "                   classifier_RandomForestClassifier,\n",
    "                   classifier_LinearSVC_Classifier]\n",
    "\n",
    "classifiers_Score=[]\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1_macro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Accuracy, Precision, Recall and F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for classifier in classifier_List:\n",
    "    scores = cross_validate(classifier, TFIDF, y_labels, cv=kf,scoring=scoring)\n",
    "#     classifiers_Score.append(scores)\n",
    "    print(\"-\"*10,classifier,\"-\"*10)\n",
    "    print(\"\\n Accuracy : \",np.average(scores['test_accuracy']))\n",
    "    print(\" Precision : \",np.average(scores['test_precision_macro']))\n",
    "    print(\" Recall : \",np.average(scores['test_recall_macro']))\n",
    "    print(\" F1_macro : \",np.average(scores['test_f1_macro']))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
